services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      # Reduced heap size for lower memory/CPU usage
      - ES_JAVA_OPTS=-Xms256m -Xmx256m -XX:+UseG1GC -XX:G1HeapRegionSize=4m -XX:MaxGCPauseMillis=200
      - xpack.security.enabled=false
      - xpack.security.enrollment.enabled=false
      # Performance optimizations for development
      - xpack.monitoring.collection.enabled=false
      - xpack.ml.enabled=false
      - xpack.watcher.enabled=false
      - xpack.graph.enabled=false
      - indices.query.bool.max_clause_count=1024
      # Reduce thread pools
      - thread_pool.write.queue_size=200
      - thread_pool.search.queue_size=200
      # Disable unnecessary features
      - cluster.routing.allocation.disk.threshold_enabled=false
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - elk
    # CPU and memory limits
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 5
      start_period: 60s

  logstash:
    image: docker.elastic.co/logstash/logstash:7.17.0
    container_name: logstash
    volumes:
      - ./logstash/pipeline:/usr/share/logstash/pipeline:ro
      - ./uploads:/data/uploads:ro
    ports:
      - "5044:5044"
      - "9600:9600"
      - "5050:5000"   # TCP streaming input
      - "8080:8080"   # HTTP webhook input
    environment:
      # Logstash 7.x works better with lower memory (increased for stability)
      - LS_JAVA_OPTS=-Xms512m -Xmx512m
      # Reduce workers and batch size
      - PIPELINE_WORKERS=1
      - PIPELINE_BATCH_SIZE=125
      - PIPELINE_BATCH_DELAY=50
    networks:
      - elk
    depends_on:
      elasticsearch:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1024M
        reservations:
          cpus: '0.25'
          memory: 512M
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9600/_node/stats || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 5
      start_period: 60s

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      # Kibana 8.x needs at least 1GB to start reliably
      - NODE_OPTIONS=--max-old-space-size=1024
      - LOGGING_QUIET=true
      - TELEMETRY_ENABLED=false
      - xpack.security.enabled=false
    networks:
      - elk
    depends_on:
      elasticsearch:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1280M
        reservations:
          cpus: '0.25'
          memory: 1024M
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5601/api/status || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 5
      start_period: 180s

  mongodb:
    image: mongo:7
    container_name: mongodb
    ports:
      - "27017:27017"
    environment:
      - MONGO_INITDB_ROOT_USERNAME=admin
      - MONGO_INITDB_ROOT_PASSWORD=password123
    volumes:
      - mongodb_data:/data/db
    networks:
      - elk
    # CPU and memory limits  
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M
    # Use wiredTiger cache size limit (minimum 0.25GB)
    command: ["--wiredTigerCacheSizeGB", "0.25"]
    healthcheck:
      test: ["CMD-SHELL", "bash -c 'echo > /dev/tcp/localhost/27017'"]
      interval: 30s
      timeout: 5s
      retries: 10
      start_period: 30s

  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    # Reduced memory and optimized for low CPU
    command: redis-server --maxmemory 64mb --maxmemory-policy allkeys-lru --save "" --appendonly no
    volumes:
      - redis_data:/data
    networks:
      - elk
    # CPU and memory limits
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M
        reservations:
          cpus: '0.05'
          memory: 32M
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5

  webapp:
    build:
      context: ./app
      dockerfile: Dockerfile
    container_name: webapp
    ports:
      - "5000:5000"
    volumes:
      - ./app:/app
    environment:
      - ELASTICSEARCH_HOST=http://elasticsearch:9200
      - MONGODB_URI=mongodb://admin:password123@mongodb:27017/
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - FLASK_ENV=production
      # Limit Flask threads
      - FLASK_RUN_THREADED=false
    networks:
      - elk
    depends_on:
      elasticsearch:
        condition: service_healthy
      mongodb:
        condition: service_healthy
      redis:
        condition: service_healthy
    # CPU and memory limits
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # ============================================================
  # Observability Stack
  # ============================================================
  
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus/alerts.yml:/etc/prometheus/alerts.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'
      - '--web.enable-lifecycle'
    networks:
      - elk
    depends_on:
      - webapp
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3

  grafana:
    image: grafana/grafana:10.2.2
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin123
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=http://localhost:3000
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
    networks:
      - elk
    depends_on:
      - prometheus
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  jaeger:
    image: jaegertracing/all-in-one:1.52
    container_name: jaeger
    ports:
      - "16686:16686"  # UI
      - "6831:6831/udp"  # Thrift compact
      - "14268:14268"  # HTTP collector
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    networks:
      - elk
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:16686/"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  elk:
    driver: bridge

volumes:
  elasticsearch_data:
    driver: local
  mongodb_data:
    driver: local
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
