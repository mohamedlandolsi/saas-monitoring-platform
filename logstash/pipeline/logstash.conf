input {
  # ============================================================
  # STREAMING INPUTS (Real-time ingestion)
  # ============================================================
  
  # TCP input for streaming logs (port 5000)
  tcp {
    port => 5000
    codec => json_lines
    tags => ["stream", "tcp"]
  }
  
  # HTTP webhook input (port 8080)
  http {
    port => 8080
    codec => json
    tags => ["stream", "webhook"]
  }
  
  # Redis Pub/Sub for distributed log streaming
  redis {
    host => "redis"
    port => 6379
    data_type => "channel"
    key => "logs"
    codec => json
    tags => ["stream", "redis"]
  }

  # ============================================================
  # FILE INPUTS (Batch ingestion)
  # ============================================================
  
  # CSV file input pipeline
  file {
    path => "/data/uploads/*.csv"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    mode => "read"
    file_completed_action => "log"
    file_completed_log_path => "/usr/share/logstash/data/completed_files.log"
    tags => ["csv"]
    discover_interval => 15
    stat_interval => 5
  }

  # JSON file input pipeline
  file {
    path => "/data/uploads/*.json"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    mode => "read"
    file_completed_action => "log"
    file_completed_log_path => "/usr/share/logstash/data/completed_files.log"
    codec => "json"
    tags => ["json"]
    discover_interval => 15
    stat_interval => 5
  }
}

filter {
  # ============================================================
  # CSV Processing Pipeline
  # ============================================================
  if "csv" in [tags] {
    csv {
      separator => ","
      skip_header => true
      columns => [
        "timestamp",
        "log_type",
        "level",
        "client_ip",
        "user_id",
        "method",
        "endpoint",
        "status_code",
        "response_time_ms",
        "user_agent",
        "message",
        "sql_query",
        "query_duration_ms",
        "server",
        "tenant_id"
      ]
    }

    # Parse timestamp
    if [timestamp] {
      date {
        match => ["timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"]
        target => "@timestamp"
        remove_field => ["timestamp"]
      }
    }

    # Convert data types
    mutate {
      convert => {
        "status_code" => "integer"
        "response_time_ms" => "float"
        "query_duration_ms" => "float"
        "user_id" => "integer"
      }
      strip => ["log_type", "level", "method", "endpoint", "server", "tenant_id"]
      add_field => { "source_type" => "csv" }
    }

    # Add geoip enrichment for client_ip if available
    if [client_ip] {
      geoip {
        source => "client_ip"
        target => "geoip"
        tag_on_failure => ["_geoip_lookup_failure"]
      }
    }
  }

  # ============================================================
  # JSON Processing Pipeline
  # ============================================================
  if "json" in [tags] {
    # Parse timestamp if exists
    if [timestamp] {
      date {
        match => ["timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"]
        target => "@timestamp"
        remove_field => ["timestamp"]
      }
    }

    # Convert data types
    mutate {
      convert => {
        "status_code" => "integer"
        "response_time_ms" => "float"
        "query_duration_ms" => "float"
      }
      add_field => { "source_type" => "json" }
    }

    # Add geoip enrichment for client_ip if available
    if [client_ip] {
      geoip {
        source => "client_ip"
        target => "geoip"
        tag_on_failure => ["_geoip_lookup_failure"]
      }
    }
  }

  # ============================================================
  # Common Enrichment and Error Handling
  # ============================================================
  
  # Add log level tags for easier filtering
  if [level] {
    mutate {
      add_tag => ["level_%{level}"]
    }
  }

  # Tag error logs for dashboards
  if [level] == "ERROR" or [level] == "CRITICAL" {
    mutate {
      add_tag => ["error_log"]
    }
  }

  # Tag slow requests (> 1000ms)
  if [response_time_ms] and [response_time_ms] > 1000 {
    mutate {
      add_tag => ["slow_request"]
    }
  }

  # Handle parsing errors
  if "_csvparsefailure" in [tags] or "_jsonparsefailure" in [tags] {
    mutate {
      add_tag => ["parse_error"]
      add_field => { "error_message" => "Failed to parse log entry" }
    }
  }
}

output {
  # Send to Elasticsearch with daily indices
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "saas-logs-%{+YYYY.MM.dd}"
  }

  # Log errors to separate index
  if "parse_error" in [tags] {
    elasticsearch {
      hosts => ["http://elasticsearch:9200"]
      index => "saas-logs-errors-%{+YYYY.MM.dd}"
    }
  }
}
